import customtkinter as ctk
import tkinter
from PIL import Image, ImageTk, ImageDraw, ImageFont, ImageSequence
import requests
from io import BytesIO
import threading
import speech_recognition as sr
import cv2
import mediapipe as mp
import numpy as np
import pickle
import os
import sys
import arabic_reshaper
from bidi.algorithm import get_display
from gtts import gTTS
import pygame
import hashlib
import time 



if not os.path.exists("audio_cache"):
    os.makedirs("audio_cache")
if os.name == 'nt':
    os.system('chcp 65001')
sys.stdout.reconfigure(encoding='utf-8')
ctk.set_appearance_mode("dark")
#ctk.set_default_color_theme("pink.json")
#   print("Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯:", os.path.exists("pink.json"))

mapping = {
      "Ø§Ù…": "https://raw.githubusercontent.com/dali22alk/my-gif/main/mom.gif",
    "Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡": "https://raw.githubusercontent.com/dali22alk/my-gif/main/alhmdullah-ezgif.com-video-to-gif-converter.gif",
    "ØºØ¶Ø¨": "https://raw.githubusercontent.com/dali22alk/my-gif/main/angry.gif",
    "Ø§ÙŠÙ† Ø§Ù„Ù…ÙƒØ§Ù†": "https://raw.githubusercontent.com/dali22alk/my-gif/main/aynaalmkan.gif",
    "Ø·ÙÙ„ Ø±Ø¶ÙŠØ¹": "https://raw.githubusercontent.com/dali22alk/my-gif/main/baby.gif",
    "Ø³ÙŠØ¡": "https://raw.githubusercontent.com/dali22alk/my-gif/main/bad.gif",
    "Ø§Ø®": "https://raw.githubusercontent.com/dali22alk/my-gif/main/brother.gif",
    "Ø§Ø¨": "https://raw.githubusercontent.com/dali22alk/my-gif/main/dad.gif",
    "Ø§Ù„Ø¹Ø§Ø¦Ù„Ù‡": "https://raw.githubusercontent.com/dali22alk/my-gif/main/family.gif",
    "Ø¬ÙŠØ¯": "https://raw.githubusercontent.com/dali22alk/my-gif/main/good.gif",
    "Ø¬Ø¯": "https://raw.githubusercontent.com/dali22alk/my-gif/main/grandfather.gif",
    "Ø¬Ø¯Ù‡": "https://raw.githubusercontent.com/dali22alk/my-gif/main/grandmom.gif",
    "Ø³Ø¹ÙŠØ¯": "https://raw.githubusercontent.com/dali22alk/my-gif/main/happy.gif",
    "Ø¨ÙŠØª": "https://raw.githubusercontent.com/dali22alk/my-gif/main/house.gif",
    "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ": "https://raw.githubusercontent.com/dali22alk/my-gif/main/howareu-ezgif.com-video-to-gif-converter.gif",
    "Ø§Ù†Ø§ Ø¨Ø®ÙŠØ±": "https://raw.githubusercontent.com/dali22alk/my-gif/main/imgood-ezgif.com-video-to-gif-converter.gif",
    "Ù‚Ù„Ù‚": "https://raw.githubusercontent.com/dali22alk/my-gif/main/kalak.gif",
    "Ù„Ùˆ Ø³Ù…Ø­Øª": "https://raw.githubusercontent.com/dali22alk/my-gif/main/lawsama7t.gif",
    "ÙŠØ­Ø¨": "https://raw.githubusercontent.com/dali22alk/my-gif/main/love.gif", 
    "Ù…Ø¨Ø§Ø±Ùƒ": "https://raw.githubusercontent.com/dali22alk/my-gif/main/mabrok.gif",
    "Ù…Ø¯Ø±Ø³Ù‡": "https://raw.githubusercontent.com/dali22alk/my-gif/main/madrasah.gif",
    "Ø­Ø²ÙŠÙ†": "https://raw.githubusercontent.com/dali22alk/my-gif/main/sad.gif",
    "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…": "https://raw.githubusercontent.com/dali22alk/my-gif/main/salamalaekom-ezgif.com-video-to-gif-converter.gif",
    "Ø´Ø±ÙƒÙ‡": "https://raw.githubusercontent.com/dali22alk/my-gif/main/shareka.gif",
    "Ø§Ø®Øª": "https://raw.githubusercontent.com/dali22alk/my-gif/main/sister.gif",
    "Ø§Ø¨Ù†": "https://raw.githubusercontent.com/dali22alk/my-gif/main/son.gif",
    "Ø§Ø³Ù": "https://raw.githubusercontent.com/dali22alk/my-gif/main/sorry.gif",
    "Ø¬Ø§Ù…Ø¹Ù‡": "https://raw.githubusercontent.com/dali22alk/my-gif/main/uni.gif",
    "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±": "https://raw.githubusercontent.com/dali22alk/my-gif/main/goodmorning-ezgif.com-video-to-gif-converter.gif",
    "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±": "https://raw.githubusercontent.com/dali22alk/my-gif/main/goodevening-ezgif.com-video-to-gif-converter.gif",
    "ÙƒÙ… Ø§Ù„Ø³Ø§Ø¹Ù‡": "https://raw.githubusercontent.com/dali22alk/my-gif/main/alsa3akm.gif"

}

stop_animation = False

def animate(frames, delay, index=0):
    global stop_animation
    if stop_animation:
        return
    frame = frames[index]
    label_gif.config(image=frame)
    label_gif.image = frame
    tabview.after(delay, animate, frames, delay, (index + 1) % len(frames))

def load_gif_from_url(word):
    global stop_animation
    stop_animation = True
    if word in mapping:
        url = mapping[word]
        try:
            response = requests.get(url)
            response.raise_for_status()
            img = Image.open(BytesIO(response.content))
            frames = [ImageTk.PhotoImage(frame.copy().convert('RGBA')) for frame in ImageSequence.Iterator(img)]
            delay = img.info.get('duration', 100)
            stop_animation = False
            animate(frames, delay)
        except Exception as e:
            label_gif.config(text=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„: {e}", image=None)
    else:
        label_gif.config(text="Ù„Ø§ ÙŠÙˆØ¬Ø¯ GIF Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø©", image=None)

def show_gif_threaded(word):
    label_gif.config(text="...Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„", image=None)
    threading.Thread(target=load_gif_from_url, args=(word,), daemon=True).start()

def use_entry_word():
    word = entry.get().strip()
    show_gif_threaded(word)

def recognize_speech():
    label_gif.config(text="...Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„", image=None)
    r = sr.Recognizer()
    with sr.Microphone() as source:
        try:
            audio = r.listen(source, timeout=5)
            text = r.recognize_google(audio, language='ar-SA')
            entry.delete(0, ctk.END)
            entry.insert(0, text)
            show_gif_threaded(text.strip())
        except sr.UnknownValueError:
            label_gif.config(text="ØªØ¹Ø°Ù‘Ø± ÙÙ‡Ù… Ø§Ù„ØµÙˆØª")
        except sr.RequestError:
            label_gif.config(text="ØªØ¹Ø°Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„")
        except Exception as e:
            label_gif.config(text=f"Ø®Ø·Ø£: {e}")

labels_dict_numbers = {
   0: 'ØµÙØ±', 1: 'ÙˆØ§Ø­Ø¯', 2: 'Ø§Ø«Ù†Ø§Ù†', 3: 'Ø«Ù„Ø§Ø«Ø©', 4: 'Ø£Ø±Ø¨Ø¹Ø©',
    5: 'Ø®Ù…Ø³Ø©', 6: 'Ø³ØªØ©', 7: 'Ø³Ø¨Ø¹Ø©', 8: 'Ø«Ù…Ø§Ù†ÙŠØ©', 9: 'ØªØ³Ø¹Ø©', 10: 'Ø¹Ø´Ø±Ø©'
}
labels_dict_letters = {
    0: 'Ø§', 1: 'Ø¨', 2: 'Øª', 3: 'Ø«', 4: 'Ø¬', 5: 'Ø­',
    6: 'Ø®', 7: 'Ø¯', 8: 'Ø°', 9: 'Ø±', 10: 'Ø²', 11: 'Ø³',
    12: 'Ø´', 13: 'Øµ', 14: 'Ø¶', 15: 'Ø·', 16: 'Ø¸', 17: 'Ø¹',
    18: 'Øº', 19: 'Ù', 20: 'Ù‚', 21: 'Ùƒ', 22: 'Ù„', 23: 'Ù…',
    24: 'Ù†', 25: 'Ù‡', 26: 'Ùˆ', 27: 'ÙŠ', 28: 'Ø§Ù„', 29: 'Ø©'
}
labels_dict_words = {
    0: 'Ø§ØªÙ…Ù†Ù‰ Ù„Ùƒ Ø­ÙŠØ§Ø© Ø³Ø¹ÙŠØ¯Ø©', 1: 'Ø£Ù†Øª', 2: 'Ø¹Ù… Ø¯',
    3: 'Ø³ÙŠØ¡', 4: 'Ù…Ø±Ø­Ø¨Ø§', 5: 'Ù‡Ø°Ø§ Ø±Ù‡ÙŠØ¨',
    6: 'Ø£Ø­Ø¨Ùƒ', 7: 'Ù„Ùˆ Ø³Ù…Ø­Øª', 8: 'Ø¬Ù…Ù„Ø© Ø§Ùˆ ÙƒÙ„Ù…Ø§Øª',
    9: 'Ø§Ù„ÙŠÙˆÙ…'
}

class SignToSpeechApp(ctk.CTkFrame):
    def __init__(self, master):
        super().__init__(master)
        self.pack(fill="both", expand=True)

        self.cap = None
        self.camera_label = None
        self.model = None
        self.running = False
        self.labels_dict = {}
        self.speech_enabled = True
        self.text_from_sign = []

        # Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ù†Ø·Ù‚
        self.last_prediction = None
        self.prediction_count = 0
        self.prediction_threshold = 7  # Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ù„ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø±Ù
        self.last_spoken = None
        self.last_spoken_time = 0
        self.min_speak_interval = 2 
         # ÙØ§ØµÙ„ Ø²Ù…Ù†ÙŠ Ø¨Ø§Ù„Ù†Ø·Ù‚ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
        self.use_left_hand = False  # Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§ Ø§Ù„ÙŠØ¯ Ø§Ù„ÙŠÙ…Ù†Ù‰

        pygame.mixer.init()

        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)
        self.drawing = mp.solutions.drawing_utils
        self.drawing_styles = mp.solutions.drawing_styles

        sidebar = ctk.CTkFrame(self, width=150)
        sidebar.pack(side="left", fill="y")

        ctk.CTkButton(sidebar,text="Ø§Ù„Ø£Ø±Ù‚Ø§Ù…", command=lambda: self.start_model('D:\\img_num\\model.p', labels_dict_numbers),height=50,width=180, font=ctk.CTkFont(size=18, weight="bold")).pack(pady=15)
        ctk.CTkButton(sidebar,text="Ø§Ù„Ø­Ø±ÙˆÙ",command=lambda: self.start_model('D:\\img_char\\model.p', labels_dict_letters),height=50,width=180,font=ctk.CTkFont(size=18, weight="bold")).pack(pady=15)
        ctk.CTkButton(sidebar,text="Ø§Ù„ÙƒÙ„Ù…Ø§Øª",command=lambda: self.start_model('D:\\img_word\\model.p', labels_dict_words), height=50,width=180, font=ctk.CTkFont(size=18, weight="bold")).pack(pady=15)
        ctk.CTkButton( sidebar, text="   Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§ÙŠÙ‚Ø§Ù",command=self.stop_camera,height=50,width=180,font=ctk.CTkFont(size=18, weight="bold")).pack(pady=15)
        ctk.CTkButton( sidebar,text=" Ø§Ù„ØµÙˆØª Ø§ÙŠÙ‚Ø§Ù/ØªØ´ØºÙŠÙ„",command=self.toggle_speech,height=50,width=180,font=ctk.CTkFont(size=18, weight="bold")).pack(pady=15)
        ctk.CTkButton(sidebar,text="ðŸ”„Ø§Ù„Ù†Øµ Ù…Ø³Ø­", command=self.clear_text, height=50, width=180,font=ctk.CTkFont(size=18, weight="bold")).pack(pady=15)
        self.hand_toggle_button = ctk.CTkButton(sidebar, text="Ø§Ù„ÙŠÙ…Ù†Ù‰ ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…", command=self.toggle_hand, height=50,  width=180, font=ctk.CTkFont(size=18, weight="bold"))
        self.hand_toggle_button.pack(pady=15)



        self.content_frame = ctk.CTkFrame(self)
        self.content_frame.pack(side="left", fill="both", expand=True, padx=20, pady=20)

        self.camera_label = ctk.CTkLabel(self.content_frame)
        self.camera_label.pack(pady=10)
        self.spoken_text_display_sign = ctk.CTkLabel(
        self.content_frame,
        text="",
        font=ctk.CTkFont(size=20),
        wraplength=900,
        anchor="e",
        justify="right"
)
        self.spoken_text_display_sign.pack(pady=10)

    def clear_text(self):
     self.text_from_sign.clear()  # ØªÙØ±ÙŠØº Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
     self.spoken_text_display_sign.configure(text="")  # Ø¥ÙØ±Ø§Øº Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø¸Ø§Ù‡Ø±
     self.last_spoken = None  # Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø¢Ø®Ø± Ù…Ø§ ØªÙ… Ù†Ø·Ù‚Ù‡
     self.prediction_count = 0
     self.last_prediction = None

    def toggle_speech(self):
        self.speech_enabled = not self.speech_enabled
    def toggle_hand(self):
     self.use_left_hand = not self.use_left_hand
     if self.use_left_hand:
        self.hand_toggle_button.configure(text="Ø§Ù„ÙŠØ³Ø±Ù‰ ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…")
     else:
        self.hand_toggle_button.configure(text="Ø§Ù„ÙŠÙ…Ù†Ù‰ ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…")

    def speak(self, text):
        if not self.speech_enabled:
            return
        try:
            filename = f"audio_cache/{hashlib.md5(text.encode('utf-8')).hexdigest()}.mp3"
            if not os.path.exists(filename):
                tts = gTTS(text=text, lang='ar')
                tts.save(filename)
            pygame.mixer.music.load(filename)
            pygame.mixer.music.play()
            while pygame.mixer.music.get_busy():
                continue
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø·Ù‚: {e}")

    def start_model(self, model_path, labels_dict):
        self.labels_dict = labels_dict
        try:
            with open(model_path, 'rb') as f:
                model_dict = pickle.load(f)
                self.model = model_dict['model']
        except Exception as e:
            print(f"Model load error: {e}")
            return

        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            print("ÙØ´Ù„ ÙÙŠ ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
            return
        self.running = True
        self.update_frame()

    def update_frame(self):
        if self.running and self.cap:
            ret, frame = self.cap.read()
            if not ret:
                return

            frame = cv2.flip(frame, 1)
            data_aux, x_, y_ = [], [], []
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(frame_rgb)

            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    self.drawing.draw_landmarks(frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                    for lm in hand_landmarks.landmark:
                        x_.append(lm.x)
                        y_.append(lm.y)
                    for lm in hand_landmarks.landmark:
                        data_aux.append(lm.x - min(x_))
                        data_aux.append(lm.y - min(y_))

                if self.model and len(data_aux) == 42:
                    # Ø¹ÙƒØ³ Ø§Ù„Ø§Ø­Ø¯Ø§Ø«ÙŠØ§Øª ÙÙŠ Ø­Ø§Ù„ ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙŠØ¯ Ø§Ù„ÙŠØ³Ø±Ù‰
                    if self.use_left_hand:
                     for i in range(0, len(data_aux), 2):
                      data_aux[i] = 1 - data_aux[i]

                    prediction = self.model.predict([np.asarray(data_aux)])
                    label = self.labels_dict.get(int(prediction[0]), '?')

                    if label == self.last_prediction:
                        self.prediction_count += 1
                    else:
                        self.last_prediction = label
                        self.prediction_count = 1

                    if self.prediction_count >= self.prediction_threshold:
                        current_time = time.time()
                        if current_time - self.last_spoken_time > self.min_speak_interval:
                            if self.last_spoken != label:
                                self.last_spoken = label
                                self.speak(label)
                                self.last_spoken_time = current_time
                                self.text_from_sign.append(label)
                                full_text = " ".join(self.text_from_sign)
                                reshaped_text = arabic_reshaper.reshape(full_text)
                                bidi_text = get_display(reshaped_text)
                                self.spoken_text_display_sign.configure(text=f" :Ø§Ù„Ù†Øµ \n {bidi_text}")



                        reshaped = arabic_reshaper.reshape(label)
                        label_display = get_display(reshaped)

                        img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                        draw = ImageDraw.Draw(img_pil)
                        try:
                            font = ImageFont.truetype("arial.ttf", 36)
                        except:
                            font = ImageFont.load_default()
                        draw.text((50, 50), label_display, font=font, fill=(0, 255, 0))
                        frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            img = img.resize((750, 550))
            imgtk = ImageTk.PhotoImage(image=img)
            self.camera_label.imgtk = imgtk
            self.camera_label.configure(image=imgtk)

            self.after(10, self.update_frame)

    def stop_camera(self):
        self.running = False
        if self.cap:
            self.cap.release()
            self.cap = None
        if self.camera_label:
            self.camera_label.configure(image='')

root = ctk.CTk()
root.geometry("1100x750")
root.title("Ù…ØªØ±Ø¬Ù… Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ù„Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø©")

tabview = ctk.CTkTabview(root)
tabview.pack(fill="both", expand=True)

tab1 = tabview.add("ØµÙˆØª â†’ Ø¥Ø´Ø§Ø±Ø©")
entry = ctk.CTkEntry(tab1, width=300)
entry.pack(pady=10)
label_gif = tkinter.Label(tab1)
label_gif.pack(pady=10)

btn_show = ctk.CTkButton(tab1, text="Ø§Ù„ØµÙˆØ±Ø© Ø¹Ø±Ø¶", command=use_entry_word, height=50, width=180, font=ctk.CTkFont(size=18, weight="bold")); btn_show.pack(pady=10)


btn_speech = ctk.CTkButton(tab1, text="Ù„Ù„ØªØ³Ø¬ÙŠÙ„ Ø£Ù†Ù‚Ø±", command=recognize_speech, height=50, width=180, font=ctk.CTkFont(size=18, weight="bold")); btn_speech.pack(pady=10)


app_tab2 = tabview.add("Ø¥Ø´Ø§Ø±Ø© â†’ ØµÙˆØª")
sign_to_speech_app = SignToSpeechApp(app_tab2)

root.mainloop()